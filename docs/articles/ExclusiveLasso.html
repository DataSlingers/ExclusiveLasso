<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Introduction to the `ExclusiveLasso` Package • ExclusiveLasso</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/flatly/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Introduction to the `ExclusiveLasso` Package">
<meta property="og:description" content="ExclusiveLasso">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">ExclusiveLasso</a>
        <span class="version label label-info" data-toggle="tooltip" data-placement="bottom" title="ExclusiveLasso is not yet on CRAN">0.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/ExclusiveLasso.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/DataSlingers/ExclusiveLasso">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Introduction to the <code>ExclusiveLasso</code> Package</h1>
                        <h4 class="author">Michael Weylandt</h4>
            <address class="author_afil">
      Department of Statistics, Rice University<br><a class="author_email" href="mailto:#"></a><a href="mailto:michael.weylandt@rice.edu" class="email">michael.weylandt@rice.edu</a>
      </address>
                  
            <h4 class="date">2018-10-23</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/DataSlingers/ExclusiveLasso/blob/master/vignettes/ExclusiveLasso.Rmd"><code>vignettes/ExclusiveLasso.Rmd</code></a></small>
      <div class="hidden name"><code>ExclusiveLasso.Rmd</code></div>

    </div>

    
    
<p><span class="math display">\[
\DeclareMathOperator{\argmin}{arg\,min}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\trace}{Trace}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bmu}{\mathbf{\mu}}
\DeclareMathOperator{\prox}{\textsf{prox}}
\]</span></p>
<p>The <code>ExclusiveLasso</code> package implements the exclusive lasso penalty of Zhou <em>et al.</em> <span class="citation">(2010)</span>, which Obozinski and Bach showed is the tightest convex relaxation of the combinatorial constraint “exactly one non-zero element in each group” <span class="citation">(2012, Section 4.2)</span>. For the Gaussian case, we implement the two algorithms discussed in Campbell and Allen <span class="citation">(2017)</span>:</p>
<ul>
<li>proximal gradient descent, with coordinate descent used to evaluate the proximal operator;</li>
<li>full coordinate descent algorithm.</li>
</ul>
<p>The scalings used in the <code>ExclusiveLasso</code> package are not exactly those used by Campbell and Allen, so we describe the algorithms used in the package in detail below.</p>
<div id="usage" class="section level2">
<h2 class="hasAnchor">
<a href="#usage" class="anchor"></a>Usage</h2>
<p>The <code>ExclusiveLasso</code> package implements the exclusive lasso penalty <span class="citation">(Zhou, Jin, and Hoi 2010; Campbell and Allen 2017)</span> for structured variable selection. The interface and internal design intentionally mimic those of the <code>glmnet</code> package <span class="citation">(Friedman, Hastie, and Tibshirani 2010)</span> and, by extension, other sparse regression packages which follow <code>glmnet</code>, notably <code>ncvreg</code> for non-convex regularization and <code>grpreg</code> for the group lasso penalty <span class="citation">(Breheny and Huang 2011, 2015)</span>.</p>
<p>We demonstrate its use on a small simulated data set:</p>
<div class="sourceCode" id="cb1"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span>(<span class="fl">1234</span>)
<span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="no">ExclusiveLasso</span>)

<span class="no">n</span> <span class="kw">&lt;-</span> <span class="fl">100</span>
<span class="no">p</span> <span class="kw">&lt;-</span> <span class="fl">100</span>
<span class="no">g</span> <span class="kw">&lt;-</span> <span class="fl">5</span>

<span class="no">groups</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span>(<span class="fl">1</span>:<span class="no">g</span>, <span class="kw">length.out</span><span class="kw">=</span><span class="no">p</span>)

<span class="no">Sig</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/toeplitz.html">toeplitz</a></span>(<span class="fl">0.7</span>^((<span class="fl">1</span>:<span class="no">p</span>) - <span class="fl">1</span>))
<span class="no">Sig_L</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/chol.html">chol</a></span>(<span class="no">Sig</span>)

<span class="no">beta</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span>(<span class="fl">0</span>, <span class="no">p</span>); <span class="no">beta</span>[<span class="fl">1</span>:<span class="no">g</span>] <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span>(<span class="no">g</span>, <span class="fl">2</span>, <span class="fl">3</span>)
<span class="no">X</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span>(<span class="no">n</span> * <span class="no">p</span>), <span class="kw">ncol</span><span class="kw">=</span><span class="no">p</span>) <span class="kw">%*%</span> <span class="no">Sig_L</span>
<span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span>(<span class="no">X</span>) <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span>(<span class="no">beta</span> <span class="kw">!=</span> <span class="fl">0</span>, <span class="st">"T"</span>, <span class="st">"F"</span>), <span class="fl">1</span>:<span class="no">p</span>)
<span class="no">y</span> <span class="kw">&lt;-</span> <span class="no">X</span> <span class="kw">%*%</span> <span class="no">beta</span> + <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span>(<span class="no">n</span>)

<span class="no">exfit</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/exclusive_lasso.html">exclusive_lasso</a></span>(<span class="no">X</span>, <span class="no">y</span>, <span class="kw">groups</span><span class="kw">=</span><span class="no">groups</span>)
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span>(<span class="no">exfit</span>)</pre></body></html></div>
<pre><code>## Exclusive Lasso Fit 
## ------------------- 
## 
## N: 100. P: 100.
## 5 groups. Median size 20 
## 
## Grid: 100 values of lambda. 
##   Miniumum: 0.0008960734 
##   Maximum:  8.960734 
##   Degrees of freedom:  0.4824523  --&gt;  83.95467 
##   Number of selected variables: 5  --&gt;  85 
## 
## Fit Options:
##   - Family:         Gaussian 
##   - Intercept:      TRUE 
##   - Standardize X:  TRUE 
##   - Algorithm:      Coordinate Descent 
## 
## Time:  2.519 secs</code></pre>
<p>Here we have significant correlation both within and between groups, but the correlation is highest between the 5 true positives.</p>
<p>When plotting regularization paths, the last variable to leave the active set is identified in the legend by default.</p>
<div class="sourceCode" id="cb3"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(<span class="no">exfit</span>)</pre></body></html></div>
<p><img src="ExclusiveLasso_files/figure-html/unnamed-chunk-3-1.png" width="672"></p>
<p>Note that variables from the same group are shown in the same color.</p>
<p>In many cases where the exclusive lasso is used, we have structural knowledge about the true sparsity pattern and know that we want to select exactly one variable from each group, so tuning <span class="math inline">\(\lambda\)</span> is not essential. If, however, we do wish to tune <span class="math inline">\(\lambda\)</span> in a data-driven manner, the <code>ExclusiveLasso</code> package provides a <span class="math inline">\(K\)</span>-fold cross-validation function:</p>
<div class="sourceCode" id="cb4"><html><body><pre class="r"><span class="no">exfit_cv</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/cv.exclusive_lasso.html">cv.exclusive_lasso</a></span>(<span class="no">X</span>, <span class="no">y</span>, <span class="kw">groups</span><span class="kw">=</span><span class="no">groups</span>, <span class="kw">parallel</span><span class="kw">=</span><span class="fl">FALSE</span>)
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(<span class="no">exfit_cv</span>)</pre></body></html></div>
<p><img src="ExclusiveLasso_files/figure-html/unnamed-chunk-4-1.png" width="672"></p>
<p>Running the exclusive lasso is typically quite fast and should not typically be necessary to run cross-validation in parallel. For large problems or problems with many groups, it may be necessary to parallize model fits. The <code>ExclusiveLasso</code> package is integrated with the <code>foreach</code> package, which provides interfaces to a number of parallelization schemes.</p>
</div>
<div id="algorithmic-details" class="section level2">
<h2 class="hasAnchor">
<a href="#algorithmic-details" class="anchor"></a>Algorithmic Details</h2>
<p>We use the “<span class="math inline">\(1/n\)</span>”-scaling for penalized regression, as well as including a factor of <span class="math inline">\(1/2\)</span> in the penalty term:</p>
<p><span class="math display">\[\hat{\beta}_{\text{EL}} = \argmin -\frac{1}{n}\sum_{i=1}^n l(y_i; \bx_i^T\beta) + \lambda\sum_{g \in \mathcal{G}} \frac{\|\beta_g\|_1^2}{2} \tag{EL-GLM} \label{eq:el-glm}\]</span></p>
<p>where <span class="math inline">\(l(y_i; \bx_i^T\beta)\)</span> is the log-likelihood of the observation <span class="math inline">\((\bx_i, y_i)\)</span>. For convenience, we will use the script <span class="math inline">\(\ell\)</span> to denote the scaled aggregate <em>negative</em> log-likelihood yielding the problem:<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p><span class="math display">\[\hat{\beta}_{\text{EL}} = \argmin \ell(\by; \bX\beta) + \lambda\sum_{g \in \mathcal{G}} \frac{\|\beta_g\|_1^2}{2}\]</span></p>
<p>In the Gaussian case, this can be simplified to <span class="math display">\[\hat{\beta}_{\text{EL}} = \argmin \frac{1}{2n}\|\by - \bX\beta\|_2^2 + \underbrace{\lambda \sum_{g \in \mathcal{G}} \frac{\|\beta_g\|_1^2}{2}}_{\lambda * P(\beta)} \tag{EL-Gaussian} \label{eq:el-gaussian}\]</span></p>
<p>We note that in the case where <span class="math inline">\(\mathcal{G} = \left\{\{1\}, \{2\}, \dots, \{p\}\right\}\)</span> – that is, there is no meaningful group structure – the penalty simplifies to the standard ridge penalty <span class="math inline">\(\frac{\lambda}{2}\|\beta\|_2^2\)</span>.</p>
<div id="proximal-gradient" class="section level3">
<h3 class="hasAnchor">
<a href="#proximal-gradient" class="anchor"></a>Proximal Gradient</h3>
<p>Campbell and Allen propose the use of a <em>proximal gradient</em> algorithm to solve Problem <span class="math inline">\(\eqref{eq:el-gaussian}\)</span>. Proximal gradient algorithms, first proposed in the sparse regression context by the “ISTA” algorithm of Daubechies <em>et al.</em> <span class="citation">(2004)</span> and later popularized by Beck and Teboulle <span class="citation">(2009)</span>, combine a gradient descent-type update for the smooth part of the objective with the proximal operator associated with the non-smooth part. They are particularly common for problems where the non-smooth part has a simple (or even closed form) solution.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> Proximal gradient have been applied to a wide-range of problems. See, <em>e.g.</em>, Parikh and Boyd <span class="citation">(2014)</span> for a recent review.</p>
<p>For a general penalized regression problem of the form <span class="math display">\[\argmin_{\beta} f(\beta) + \lambda g(\beta)\]</span> where <span class="math inline">\(f(\cdot)\)</span> is convex and smooth and <span class="math inline">\(g(\cdot)\)</span> is convex, but not smooth, proximal gradient algorithms work by iterating <span class="math display">\[\beta^{(k)} = \prox_{t_k \lambda g}\left(\beta^{(k-1)} - t_k \nabla f(\beta^{(k-1)}\right)\]</span> until convergence, where <span class="math inline">\(t_k\)</span> is a step-size which may be fixed or chosen by a line-search method, and <span class="math inline">\(\prox\)</span> is the proximal operator of <span class="math inline">\(t_k \lambda g\)</span>: <span class="math display">\[\prox_{t_k \lambda g}(z) = \argmin_x t_k \lambda g(x) + \frac{1}{2}\|x - z\|_2^2\]</span></p>
<p>Note that, unlike the use of the proximal operator in ADMM and similar methods <span class="citation">(Boyd et al. 2011)</span>, the step-size <span class="math inline">\(t_k\)</span> appears in the proximal operator.</p>
<p>The name “ISTA” comes from the proximal operator of the <span class="math inline">\(\ell_1\)</span>-norm: <span class="math display">\[\prox_{\lambda |\cdot|}(z) = \argmin_x \lambda |x| + \frac{1}{2}(x - z)^2\]</span> which is the so-called “soft-thresholding” operator: <span class="math display">\[\mathcal{S}_{\lambda}(x) = \begin{cases} x - \lambda &amp; x &gt; \lambda \\ 0 &amp; |x| \leq \lambda \\ x + \lambda &amp; x &lt; -\lambda \end{cases}\]</span> which gives the iterative soft-thresholding algorithm: <span class="math display">\[\beta^{(k)} = \mathcal{S}_{t\lambda}\left(\beta^{(k-1)} - \frac{t}{n}\bX^T(y - \bX\beta^{(k-1)})\right) = \mathcal{S}_{t\lambda}\left((\bI + t \bX^T\bX/n)\beta^{(k-1)} - t\bX^T\by/n\right)\]</span> where <span class="math inline">\(t = 1 / \lambda_{\text{max}}(X^TX/n)\)</span> is a constant step-size ensuring convergence.</p>
<p>Hence, for Problem <span class="math inline">\(\eqref{eq:el-gaussian}\)</span>, the proximal gradient algorithm becomes <span class="math display">\[\beta^{(k)} = \prox_{t \lambda P(\cdot)}\left((\bI + t\bX^T\bX/n)\beta^{(k-1)} - t\bX^T\by/n\right)\]</span></p>
<p>Unlike the standard <span class="math inline">\(\ell_1\)</span>-penalization case, this proximal operator cannot be evaluated in closed form and an iterative algorithm must be used to approximate the proximal operator. This gives rise to a so-called <em>inexact proximal gradient</em> scheme, convergence conditions of which were analyzed by Schmidt <em>et al.</em> <span class="citation">(2011)</span>. Campbell and Allen <span class="citation">(2017)</span> propose the use of a coordinate-descent scheme to evaluate the proximal operator, described in more detail below.</p>
<div id="evaluation-of-the-proximal-operator" class="section level4">
<h4 class="hasAnchor">
<a href="#evaluation-of-the-proximal-operator" class="anchor"></a>Evaluation of the Proximal Operator</h4>
<p>We wish to evaluate the proximal operator: <span class="math display">\[\argmin_{x} \frac{1}{2}\|x - z\|_2^2 + \frac{c}{2}\sum_{g \in \mathcal{G}} \|x_g\|_1^2\]</span> Note that we fold the constant terms into a single term <span class="math inline">\(c\)</span> in this subsection.</p>
<p>Campbell and Allen <span class="citation">(2017)</span> propose the use of coordinate descent <span class="citation">(Shi et al. 2016)</span> for this problem and show that it converges, using the sufficient conditions of Tseng <span class="citation">(2001)</span>.</p>
<p>We first note that the proximal operator can be split according to the group structure:<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> <span class="math display">\[\frac{1}{2}\|x - z\|_2^2 + \frac{c}{2}\sum_{g \in \mathcal{G}} \|x_g\|_1^2 = \sum_{g \in \mathcal{G}} \frac{1}{2}\|x_g - z_g\|_2^2 + \frac{c}{2}\|x_g\|_1^2 \]</span> so it suffices to derive the coordinate updates for the simpler problem: <span class="math display">\[\frac{1}{2}\|x - z\|_2^2 + \frac{c}{2}\|x\|_1^2\]</span> Without loss of generality, we derive the update formula for <span class="math inline">\(x_1\)</span>: <span class="math display">\[\argmin_{x_1} \frac{1}{2}\|x - z\|_2^2 + \frac{c}{2} \|x\|_1^2 \implies \argmin_{x_1} \frac{1}{2}(x_1 - z_1)^2 + \frac{c}{2}\left(2|x_1| * \|x_{-1}\|_1 + x_1^2\right)\]</span> Re-grouping and adjusting constant terms, this becomes: <span class="math display">\[\begin{align*}
&amp; \argmin_{x_1} \frac{1}{2}(x_1 - z_1)^2 + \frac{c}{2}\left(2|x_1| * \|x_{-1}\|_1 + x_1^2\right) \\
\implies &amp;\argmin_{x_1} \frac{x_1^2 - 2x_1z_1 + z_1^2 + 2c\|x_{-1}\|_1 * |x_1| + cx_1^2}{2} \\
\implies &amp;\argmin_{x_1} \frac{(1+c)x_1^2 - 2x_1z_1 + z_1^2 + 2c\|x_{-1}\|_1 * |x_1|}{2} \\
\implies &amp;\argmin_{x_1} \frac{1}{2} 2c\|x_{-1}\|*|x_1| + \frac{1}{2}\left[(1+c)x_1^2 - 2x_1z_1\right] \\
\implies &amp;\argmin_{x_1} \frac{c}{1+c}\|x_{-1}\|*|x_1| + \frac{1}{2(1+c)}\left[(1+c)x_1^2 - 2x_1z_1\right] \\
\implies &amp;\argmin_{x_1} \frac{c}{1+c}\|x_{-1}\|*|x_1| + \frac{1}{2}\left[x_1^2 - 2x_1\frac{z_1}{1+c}\right] \\
\implies &amp;\argmin_{x_1} \frac{c}{1+c}\|x_{-1}\|_1 * |x_1| + \frac{1}{2}\left[x_1^2 - 2x_1 \frac{z_1}{1+c} + \frac{z_1^2}{(1+c)^2}\right] \\
\implies &amp;\argmin_{x_1} \frac{c}{1+c} \|x_{-1}\|_1 * |x_1| + \frac{1}{2}\left(x_1 - \frac{1}{1+c}z_1\right)^2
\end{align*}\]</span> which we recognize as the proximal operator of the scaled absolute value function <span class="math display">\[f(y) = \frac{c}{1+c}\|x_{-1}\|_1 * |y|\]</span> evaluated at <span class="math inline">\(\frac{z_1}{1+c}\)</span> giving the coordinate update: <span class="math display">\[x_i \leftarrow \mathcal{S}_{\frac{c}{1 + c} \|x^{-i}\|}\left(\frac{1}{1 + c} z_i\right)\]</span></p>
<p>Pulling the common <span class="math inline">\((1+c)^{-1}\)</span> term out we get: <span class="math display">\[x_i \leftarrow \frac{1}{1+c} \mathcal{S}_{c\|x^{-i}\|}\left(z_i\right)\]</span></p>
</div>
<div id="combined-algorithm" class="section level4">
<h4 class="hasAnchor">
<a href="#combined-algorithm" class="anchor"></a>Combined Algorithm</h4>
<p>Putting these pieces together, we get the following proximal gradient algorithm for <span class="math inline">\(\eqref{eq:el-gaussian}\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Initialize
<ul>
<li><span class="math inline">\(\beta^{(0)} = 0\)</span></li>
<li><span class="math inline">\(t = \lambda_{\text{max}}(\bX^T\bX/n)^{-1}\)</span></li>
<li><span class="math inline">\(k = 1\)</span></li>
</ul>
</li>
<li>Repeat until convergence:
<ul>
<li><span class="math inline">\(\bz^{(k-1)} = (\bI + t \bX^T\bX/n)\beta^{(k-1)} - t \bX^T\by/n\)</span></li>
<li>For each group <span class="math inline">\(g \in \mathcal{G}\)</span>:
<ul>
<li>Initialize <span class="math inline">\(x_g = 0\)</span>
</li>
<li>Repeat until convergence, looping over each <span class="math inline">\(i \in g\)</span>: <span class="math display">\[x_{g, i} = \mathcal{S}_{\frac{2t\lambda}{1 + t\lambda}\|x_{g, -i}\|_1}\left(\frac{t\lambda}{1+t\lambda} z_{g, i}\right)\]</span>
</li>
</ul>
</li>
<li><span class="math inline">\(\beta^{(k)} = x\)</span></li>
<li>
<span class="math inline">\(k := k + 1\)</span>.</li>
</ul>
</li>
</ol>
<p>Generalizing slightly, if we have a vector of offsets <span class="math inline">\(\bo\)</span> and a (diagonal) weight matrix <span class="math inline">\(\bW\)</span> and an intercept term <span class="math inline">\(\alpha\)</span>, the combined algorithm becomes:</p>
<ol style="list-style-type: decimal">
<li>Initialize
<ul>
<li><span class="math inline">\(\beta^{(0)} = 0\)</span></li>
<li><span class="math inline">\(t = \lambda_{\text{max}}(\bX^T\bW\bX/n)^{-1}\)</span></li>
<li><span class="math inline">\(k = 1\)</span></li>
</ul>
</li>
<li>Repeat until convergence:
<ul>
<li><span class="math inline">\(\bz^{(k-1)} = (\bI + t \bX^T\bW\bX/n)\beta^{(k-1)} - t \bX^T\bW(\by-\bo)/n\)</span></li>
<li>For each group <span class="math inline">\(g \in \mathcal{G}\)</span>:
<ul>
<li>Initialize <span class="math inline">\(x_g = 0\)</span>
</li>
<li>Repeat until convergence, looping over each <span class="math inline">\(i \in g\)</span>: <span class="math display">\[x_{g, i} = \mathcal{S}_{\frac{2t\lambda}{1 + t\lambda}\|x_{g, -i}\|_1}\left(\frac{t\lambda}{1+t\lambda} z_{g, i}\right)\]</span>
</li>
</ul>
</li>
<li><span class="math inline">\(\beta^{(k)} = x\)</span></li>
<li><span class="math inline">\(\alpha^{(k)} = \left\langle \text{diag}(\bW)/n, \by - \bo - \bX\beta\right\rangle\)</span></li>
<li>
<span class="math inline">\(k := k + 1\)</span>.</li>
</ul>
</li>
</ol>
<p>If we wish to include box constraints of the form <span class="math inline">\(l_i \leq \beta_i \leq u_i\)</span>, we can simply directly impose these after each soft-thresholding step.</p>
</div>
</div>
<div id="coordinate-descent" class="section level3">
<h3 class="hasAnchor">
<a href="#coordinate-descent" class="anchor"></a>Coordinate Descent</h3>
<p>Campbell and Allen <span class="citation">(2017)</span> also propose using a coordinate descent algorithm to solve <span class="math inline">\(\eqref{eq:el-gaussian}\)</span> directly, similar to that used by Friedman <em>et al.</em> in the <code>glmnet</code> package <span class="citation">(2007; 2010)</span> and by Wu and Lange <span class="citation">(2008)</span>, among several others, for lasso regression. As with the proximal operator, this is slightly more complicated because the penalty term is non-separable, but they show that coordinate descent converges for the general problem (of which the proximal operator is a special case), again using the analysis of Tseng <span class="citation">(2001)</span>.</p>
<p>Coordinate descent works by sequentially selecting one variable to update and fixing all others temporarily, minimizing the objective as a function of the selected variable, and cycling through all variables until convergence. In spite of its simple structure, many variations of coordinate descent are possible, depending on how the internal minimization is solved, the strategy by which the active is chosen, <em>etc.</em> Shi <em>et al.</em> <span class="citation">(2016)</span> review a number of variants. In the case of sparse regression, significant speed-ups can also be obtained by use of an “active-set” strategy, where non-zero variables are updated more frequently than zero variables.</p>
<p>The coordinate updates for the general problem generalize those used in the proximal operator. Suppose we wish to update <span class="math inline">\(\beta_i\)</span> where <span class="math inline">\(i \in g\)</span>. Then we solve: <span class="math display">\[\argmin_{\beta_i} \frac{1}{2n} \|\by - \bX_{-i}\beta_{-i} - \bx_i\beta_i\|_2^2 + \frac{\lambda}{2}\sum_{g \in \mathcal{G}} \|\beta_g\|_1^2\]</span> We let <span class="math inline">\(\br = \by - \bX_{-i}\beta_{-i}\)</span> be the “working residual” and omit penalty terms that do not have <span class="math inline">\(\beta_i\)</span> in them: <span class="math display">\[\argmin_{\beta_i} \frac{1}{2n} \|\br - \bx_i\beta_i\|_2^2 + \frac{\lambda}{2}\left(|\beta_i| + \|\beta_{g,-i}\|_1\right)^2\]</span> Using similar algebra as before: <span class="math display">\[\begin{align*}
&amp;\argmin_{\beta_i} \frac{1}{2n} \|\br - \bx_i\beta_i\|_2^2 + \frac{\lambda}{2}\left(|\beta_i| + \|\beta_{g,-i}\|_1\right)^2 \\
\implies &amp; \argmin_{\beta_i} \frac{1}{2} \|\br - \bx_i\beta_i\|_2^2 + \frac{n\lambda}{2}\left(|\beta_i| + \|\beta_{g,-i}\|_1\right)^2 \\
\implies &amp; \argmin_{\beta_i} \frac{\|\br\|_2^2 - 2\br^T\bx_i\beta_i + \|\bx_i\|_2^2\beta_i^2 + n\lambda(\beta_i^2 + 2|\beta_i| * \|\beta_{g,-i}\|_1 + \|\beta_{g,-i}\|_1^2)}{2} \\
\implies &amp; \argmin_{\beta_i} \frac{\beta_i^2 (\|\bx_i\|_2^2 + n\lambda) - 2\br^T\bx_i\beta_i + 2n\lambda |\beta_i| * \|\beta_{g, -i}\|_1}{2} \\
\implies &amp; \argmin_{\beta_i} n\lambda\|\beta_{g,-i}\|_1 * |\beta_i| +\frac{1}{2}\left(\beta_i^2 (\|\bx_i\|_2^2 + n\lambda) - 2\br^T\bx_i\beta_i \right) \\
\implies &amp; \argmin_{\beta_i} \frac{n\lambda\|\beta_{g,-i}\|_1}{\|\bx_i\|_2^2 + n\lambda} * |\beta_i| +\frac{1}{2(\|\bx_i\|_2^2 + n\lambda)}\left(\beta_i^2 (\|\bx_i\|_2^2 + n\lambda) - 2\br^T\bx_i\beta_i \right) \\
\implies &amp; \argmin_{\beta_i} \frac{n\lambda\|\beta_{g-i}\|_1}{\|\bx_i\|_2^2 + n\lambda} * |\beta_i| +\frac{1}{2}\left(\beta_i^2 - 2\frac{\br^T\bx_i}{(\|\bx_i\|_2^2 + n\lambda)}\beta_i \right) \\
\implies &amp; \argmin_{\beta_i} \frac{n\lambda\|\beta_{g,-i}\|_1}{\|\bx_i\|_2^2 + n\lambda} * |\beta_i| +\frac{1}{2}\left(\beta_i - \frac{\br^T\bx_i}{(\|\bx_i\|_2^2 + n\lambda)} \right)^2 \\
\end{align*}\]</span> which we recognize as the proximal operator of the scaled absolute value function <span class="math display">\[f(y) = \frac{n\lambda\|\beta_{g-i}\|_1}{\|\bx_i\|_2^2 + n\lambda} * |y|\]</span> evaluated at <span class="math inline">\(\frac{\br^T\bx_i}{\|\bx_i\|_2^2 + n\lambda}\)</span>, which gives the coordinate update: <span class="math display">\[\beta_i \leftarrow \mathcal{S}_{\frac{n\lambda\|\beta_{g,-i}\|_1}{\|\bx_i\|_2^2 + n\lambda}}\left(\frac{\br^T\bx_i}{\|\bx_i\|_2^2 + n\lambda}\right)\]</span> Factoring out the common denominator, we get: <span class="math display">\[\beta_i \leftarrow \frac{1}{\|\bx_i\|_2^2 + n\lambda}\mathcal{S}_{n\lambda\|\beta_{g,-i}\|_1}\left(\br^T\bx_i\right)\]</span></p>
<p>Hence the coordinate descent algorithm is given by:</p>
<ol style="list-style-type: decimal">
<li>Initialize
<ul>
<li><span class="math inline">\(\beta = 0\)</span></li>
<li><span class="math inline">\(\br = \by - \bX\beta = \by\)</span></li>
</ul>
</li>
<li>Precompute:
<ul>
<li>
<span class="math inline">\(\bu\)</span> where <span class="math inline">\(u_i = \bx_i^T\bx_i\)</span>
</li>
</ul>
</li>
<li>Repeat until convergence:
<ul>
<li>For <span class="math inline">\(i \in [p]\)</span>:
<ul>
<li>Update working residual: <span class="math inline">\(\br := \br + \bX_i\beta_i\)</span>
</li>
<li>Set <span class="math inline">\(z = \br^T\bx_i\)</span>, <span class="math inline">\(\tilde{\lambda} = n\lambda \|\beta_{g,-i}\|_1\)</span>
</li>
<li>Update: <span class="math display">\[\beta_i = \frac{1}{u_i + n\lambda} \mathcal{S}_{\tilde{\lambda}}(z)\]</span>
</li>
<li>Update working residual: <span class="math inline">\(\br := \br - \bX_i\beta_i\)</span>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>In practice, when solving along a grid of values of <span class="math inline">\(\lambda\)</span>, convergence can be much improved by “warm-starting” <span class="math inline">\(\beta^{(0)}\)</span> at the solution for a nearby value of <span class="math inline">\(\lambda\)</span>.</p>
<p>For the general case with offsets <span class="math inline">\(\bo\)</span> and a (diagonal) weight matrix <span class="math inline">\(\bW\)</span>, as well as an unpenalized intercept <span class="math inline">\(\alpha\)</span>, the combined algorithm becomes:</p>
<ol style="list-style-type: decimal">
<li>Initialize
<ul>
<li><span class="math inline">\(\beta = 0\)</span></li>
<li><span class="math inline">\(\br = \by - \bo - \bX\beta = \by - \bo\)</span></li>
</ul>
</li>
<li>Precompute:
<ul>
<li><span class="math inline">\(\bu = \text{diag}(\bX^T\bW\bX)\)</span></li>
</ul>
</li>
<li>Repeat until convergence:
<ul>
<li>For <span class="math inline">\(i \in [p]\)</span>:
<ul>
<li>Update working residual: <span class="math inline">\(\br := \br + \bX_i\beta_i\)</span>
</li>
<li>Set <span class="math inline">\(z = \br^T\bW\bx_i\)</span>, <span class="math inline">\(\tilde{\lambda} = n\lambda \|\beta_{g,-i}\|_1\)</span>
</li>
<li>Update: <span class="math display">\[\beta_i = \frac{1}{u_i + n\lambda} \mathcal{S}_{\tilde{\lambda}}(z)\]</span>
</li>
<li>Update working residual: <span class="math inline">\(\br := \br - \bX_i\beta_i\)</span>
</li>
</ul>
</li>
<li>Update intercept:
<ul>
<li>Update working residual: <span class="math inline">\(\br: \br + \alpha 1_n\)</span>
</li>
<li>Update intercept: <span class="math inline">\(\alpha := \langle \br, \text{diag}(\bW)/n \rangle\)</span>
</li>
<li>Update working residual: <span class="math inline">\(\br := \br - \alpha 1_{n}\)</span>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>If we wish to include box constraints of the form <span class="math inline">\(l_i \leq \beta_i \leq u_i\)</span>, we can simply directly impose these after each soft-thresholding step.</p>
</div>
</div>
<div id="generalized-linear-models-with-the-exclusive-lasso-penalty" class="section level2">
<h2 class="hasAnchor">
<a href="#generalized-linear-models-with-the-exclusive-lasso-penalty" class="anchor"></a>Generalized Linear Models with the Exclusive Lasso Penalty</h2>
<p>The above methods can be generalized to arbitrary generalized linear models, though the <code>ExclusiveLasso</code> package only currently supports binomial (logistic) and Poisson GLMs. We implement a proximal gradient method with back-tracking, similar to that used for Gaussian linear models, which we describe below.</p>
<p>Before we describe the algorithm in detail, we note a useful general result which covers all the GLMs we consider in this package<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> (proof below):</p>
<ul>
<li>The gradient of the negative log-likelihood can be written as: <span class="math display">\[\nabla_{\beta} \ell = -\bX^T (\bW/n) (\by - \bmu) \in \mathbb{R}^{p}\]</span>
</li>
</ul>
<p>where: - <span class="math inline">\(\bX\)</span> is the design matrix; - <span class="math inline">\(\bW\)</span> is a diagonal matrix of observation weights; - <span class="math inline">\(\by\)</span> is the vector of observed responses; and - <span class="math inline">\(\bmu\)</span> is the vector of predicted responses.</p>
<p>As we will see below, <span class="math inline">\(\bmu\)</span> typically has the simple form <span class="math inline">\(g(\bX\beta + \bo)\)</span>, where <span class="math inline">\(g(\cdot)\)</span> depends only on the GLM family being considered.</p>
<p>To show this, we recall that the general negative log-likelihood for a GLM can be written as: <span class="math display">\[\ell = \frac{1}{n}\sum_{i=1}^n w_i l(y_i; \bx_i^T\beta + o_i)\]</span> where <span class="math inline">\(l\)</span> comes from the negative log-likehood of an an exponential family distribution and hence has the form: <span class="math display">\[l_i = b(\bx_i^T\beta + o_i) - y_i * (\bx_i^T\beta + o_i)\]</span> Taking derivatives with respect to <span class="math inline">\(\beta_j\)</span>, we see that <span class="math display">\[\frac{\partial l_i}{\partial \beta_j} = b'(\bx_i^T \beta + o_i)x_{ij} - y_ix_{ij}\]</span> Writing this out in a vector form, we see that <span class="math display">\[\nabla_{\beta} l_i = b'(\bx_i\beta + o_i)\bx_i^T - y_i \bx_i^T = \bx_i^T \left[b'(\bx_i^T\beta + o_i) - y_i\right]\]</span> Hence, since <span class="math inline">\(\ell\)</span> is just a weighted sum of <span class="math inline">\(l_i\)</span>, we get <span class="math display">\[\nabla_{\beta} \ell = \frac{1}{n}\sum_{i=1} w_i \nabla_{\beta} l_i = \bX^T\bW\left[b'(\bX\beta + \bo) - \by\right]/n = -\bX^T(\bW/n)(\by - \mu)\]</span> which shows that <span class="math inline">\(b'\)</span> is just <span class="math inline">\(g\)</span> from above.</p>
<p>We can recover the classical Gaussian model by taking <span class="math inline">\(b(x) = \frac{1}{2}x^2\)</span>, which gives <span class="math inline">\(g(x) = b'(x) = x\)</span>. This, in turn, gives the gradient <span class="math display">\[\nabla_{\beta}\ell = -\bX^T(\bW/n)(\by - \bX^T\beta - \bo)\]</span> which we would obtain by taking the gradient of the Gaussian loss directly. Solving the stationary conditions gives <span class="math display">\[\nabla_{\beta}\ell = \bX^T(\bW/n)\left[\bX\beta + \bo - \by\right] = 0 \implies \beta = (\bX^T\bW\bX)^{-1}\bX^T\bW(\by - \bo)\]</span> as one would expect.</p>
<p>For logistic regression, we take <span class="math inline">\(b(x) = \log(1 + e^x)\)</span>, and so we recover <span class="math display">\[g(x) = b'(x) = \frac{e^x}{1+e^x} = \frac{1}{1+e^{-x}}\]</span> which maps the real line to the <span class="math inline">\([0, 1]\)</span> interval.</p>
<p>Finally, for Poisson regression, we take <span class="math inline">\(b(x) = \exp(x)\)</span>, and so we recover <span class="math display">\[g(x) = b'(x) = e^x\]</span> which maps the real line to the positive half-line.</p>
<p>Consolidating these results, we see:</p>
<table class="table">
<colgroup>
<col width="13%">
<col width="43%">
<col width="43%">
</colgroup>
<thead><tr class="header">
<th align="right">Family</th>
<th align="left">Negative Log-Likelihood, <span class="math inline">\(\ell\)</span>
</th>
<th align="left">Gradient, <span class="math inline">\(\nabla_{\beta} \ell\)</span>
</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">Gaussian</td>
<td align="left"><span class="math inline">\(\frac{(\by - \bX\beta - \bo)^T\bW(\by - \bX\beta - \bo)}{2n}\)</span></td>
<td align="left"><span class="math inline">\(\frac{-\bX^T\bW(\by - \bX\beta - \bo)}{n}\)</span></td>
</tr>
<tr class="even">
<td align="right">Logistic</td>
<td align="left"><span class="math inline">\(\langle \text{diag}(\bW)/n, \log(1 + \text{exp}(\bX\beta + \bo)) - \by \odot (\bX\beta + \bo)\rangle\)</span></td>
<td align="left"><span class="math inline">\(-\bX^T\bW(\by - \textsf{inv-logit}(\bX\beta + \bo))/n\)</span></td>
</tr>
<tr class="odd">
<td align="right">Poisson</td>
<td align="left"><span class="math inline">\(\langle \text{diag}(\bW)/n, \exp(\bX\beta + \bo) - \by \odot (\bX\beta + \bo)\rangle\)</span></td>
<td align="left"><span class="math inline">\(-\bX^T\bW(\by - \text{exp}(\bX\beta + \bo))/n\)</span></td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(\odot\)</span> is the elementwise (Hadamard) product of vectors.</p>
<div id="proximal-gradient-algorithm" class="section level3">
<h3 class="hasAnchor">
<a href="#proximal-gradient-algorithm" class="anchor"></a>Proximal Gradient Algorithm</h3>
<p>The proximal gradient algorithm that we use for GLMs is similar to that for the Gaussian case, but we introduce a back-tracking step instead of using a fixed step size. In particular, we use the back-tracking scheme described by Beck and Teboulle <span class="citation">(2010)</span>, which also appears in Section 4.2 of Parikh and Boyd <span class="citation">(2014)</span>. The treatment of the unpenalized intercept term is not as simple for generalized linear models (since we do not have a closed form for the intercept in terms of the residuals), so we instead include it as an unpenalized column of <span class="math inline">\(\bX\)</span> if an intercept appears in the model.</p>
<p>The final algorithm is then:</p>
<ol style="list-style-type: decimal">
<li>Initialize
<ul>
<li><span class="math inline">\(\beta^{(0)} = 0\)</span></li>
<li><span class="math inline">\(L = \lambda_{\text{max}}(\bX^T\bW\bX/n)^{-1}\)</span></li>
</ul>
</li>
<li>Repeat until convergence:
<ul>
<li>Set: <span class="math inline">\(t = L\)</span>
</li>
<li>Calculate <span class="math inline">\(\nabla \beta^{(k-1)} = -\bX^T(\bW/n)(\by - g(\bX\beta^{(k-1)} + \bo))\)</span>
</li>
<li>Repeat:
<ul>
<li>Let <span class="math inline">\(\bz = \textsf{prox}_{t \lambda}(\beta^{(k-1)} - t \nabla \beta^{(k-1)})\)</span>
</li>
<li>If <span class="math inline">\(\ell(z) \leq \ell(\beta^{(k-1)}) + \langle \nabla \beta^{(k-1)}, z - \beta^{(k-1)} \rangle + \frac{1}{2t} \|z - \beta^{(k-1)}\|_2^2\)</span>:
<ul>
<li><span class="math inline">\(\texttt{break}\)</span></li>
</ul>
</li>
<li>Else:
<ul>
<li>Set <span class="math inline">\(t = \alpha * t\)</span> for fixed <span class="math inline">\(\alpha\)</span><a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>
</li>
</ul>
</li>
</ul>
</li>
<li>Set: <span class="math inline">\(\beta^{(k)} = z\)</span>, <span class="math inline">\(k: = k + 1\)</span>
</li>
</ul>
</li>
</ol>
<p>where <span class="math inline">\(\ell\)</span> is the smooth part of the objective function (<em>i.e.</em>, the negative log-likelihood), and the proximal operator is evaluated using the coordinate-descent scheme described above.</p>
<p>Box constraints can be imposed in the proximal operator, as described above.</p>
</div>
<div id="coordinate-descent-algorithm" class="section level3">
<h3 class="hasAnchor">
<a href="#coordinate-descent-algorithm" class="anchor"></a>Coordinate Descent Algorithm</h3>
<p>In addition to the proximal gradient scheme, we can also employ a (variant of the) (inexact) proximal Newton method <span class="citation">(Lee, Sun, and Saunders 2014)</span>. This method, sometimes also called a <em>successive quadratic approximation</em> approach <span class="citation">(Byrd, Nocedal, and Oztoprak 2016)</span>, works by replacing the smooth portion of the objective function (the negative log-likelihood) with a quadratic approximation and minimizing the approximation, typically with an iterative algorithm. Once the inner algorithm converges, the approximation is updated and the process is repeated until convergence. This method has been applied to great effect in the <code>glmnet</code> package for <span class="math inline">\(\ell_1\)</span>-penalized GLMs <span class="citation">(Friedman, Hastie, and Tibshirani 2010)</span>.</p>
<p>Quadratic approximation methods are particularly attractive for generalized linear models because they reduce to solving a series of penalized weighted linear regression problems (similar to the IRLS method which solves unpenalized GLMs by successively fitting weighted least squares) and allow us to use the efficient algorithm we have already developed for the linear case, namely coordinate descent. Unlike <code>glmnet</code>, we include a back-tracking step to guarantee descent in difficult problems.</p>
<p>As with proximal gradient, the implementation of this algorithm is made much easier by taking advantage of the structure of GLMs. In particular, we note that the Hessian (second derivative) again has a simple form. Under the same assumptions as before, we have:</p>
<p><span class="math display">\[\frac{\partial l_i}{\partial \beta_j} = b'(\bx_i^T\beta + \bo_i)x_{ij} - y_ix_{ij}\]</span></p>
<p>Taking a second derivative with respect to <span class="math inline">\(\beta_k\)</span>, we get:</p>
<p><span class="math display">\[\frac{\partial^2 l_i}{\partial \beta_j \partial \beta_k} = b''(\bx_i^T\beta + \bo_i)x_{ij}x_{ik}\]</span></p>
<p>This lets us write the general GLM hessian in a particularly simple form:</p>
<p><span class="math display">\[\mathcal{H}_{\beta} l_i = \bX^T \text{diag} \widetilde{\bW}_i \bX\]</span></p>
<p>where <span class="math inline">\(\widetilde{\bW}_i\)</span> is a matrix with <span class="math inline">\(b''(\bx_i^T\beta + \bo_i)\)</span> in the <span class="math inline">\((i, i)\)</span>-th element and zero otherwise. Hence,</p>
<p><span class="math display">\[\mathcal{H}_{\beta} \ell = \sum_{i=1}^n \frac{w_i}{n}\mathcal{H}_{\beta}l_i = \bX^T\left[\bW\widetilde{\bW}/n\right]_i\bX\]</span></p>
<p>where <span class="math inline">\(\widetilde{\bW} = \sum_{i = 1}^n \widetilde{\bW}_i\)</span>, <span class="math inline">\(\bW\)</span> is the diagonal matrix of observation weights.</p>
<p>For our sequential quadratic approximation / proximal Newton updates, we replace the smooth portion of the problem with a quadratic approximation, giving:</p>
<p><span class="math display">\[\argmin_{\beta} \ell(\by, \bX\beta) + \lambda P(\beta) \approx \argmin_{\beta} \ell(\beta_0) + \langle\nabla_{\beta = \beta_0}\ell, \beta - \beta_0\rangle + \frac{(\beta - \beta_0)^T (\mathcal{H}_{\beta = \beta_0}\ell)(\beta - \beta_0)}{2} + \lambda P(\beta)\]</span></p>
<p>which can be slightly simplified to:</p>
<p><span class="math display">\[\argmin_{\beta} \ell(\beta_0) + \langle\nabla_{\beta = \beta_0}\ell - \mathcal{H}_{\beta = \beta_0}\beta_0, \beta \rangle + \frac{\beta^T (\mathcal{H}_{\beta = \beta_0}\ell)\beta}{2} + \lambda P(\beta)\]</span></p>
<p>Using our above expressions for <span class="math inline">\(\nabla \ell\)</span> and <span class="math inline">\(\mathcal{H} \ell\)</span>, this becomes</p>
<p><span class="math display">\[\begin{align*}
\argmin_{\beta} \ell(\beta_0) + \langle\nabla_{\beta = \beta_0}\ell - \mathcal{H}_{\beta = \beta_0}\beta_0, \beta \rangle + \frac{\beta^T (\mathcal{H}_{\beta = \beta_0}\ell)\beta}{2} + \lambda P(\beta) \\
\argmin_{\beta} \ell(\beta_0) + \langle -\bX^T\bW(\by - g(\bX\beta_0 + \bo))/n - \bX^T\bW\widetilde{\bW}/n\bX\beta_0, \beta\rangle + \frac{\beta^T\bX^T\bW\widetilde{\bW}\bX\beta}{2n} + \lambda P(\beta)\\
\argmin_{\beta} \ell(\beta_0) -\frac{(\by - g(\bX\beta_0 + \bo) - \widetilde{\bW}\bX\beta_0)^T\bW\bX\beta}{n} + \frac{\beta^T\bX^T\bW\widetilde{\bW}\bX\beta}{2n} + \lambda P(\beta) \\
\argmin_{\beta} \ell(\beta_0) -\frac{\left(\widetilde{\bW}^{-1}(\by - g(\bX\beta_0 + \bo) - \bX\beta_0\right)^T\bW\widetilde{\bW}\bX\beta}{n} + \frac{\beta^T\bX^T\bW\widetilde{\bW}\bX\beta}{2n} + \lambda P(\beta) \\
\end{align*}\]</span></p>
<p>We note that this quadratic approximation is of the same form as a weighted Gaussian likelihood</p>
<p><span class="math display">\[\argmin_{\beta} \frac{(\by - \bX\beta)^T\bW(\by - \bX\beta)}{2n} + \lambda P(\beta) = \argmin_{\beta}\frac{\by^T\bW\by}{2n} - \frac{\by^T\bW\bX\beta}{2n} + \frac{\beta^T\bX^T\bW\bX^T\beta}{n} + \lambda P(\beta)\]</span></p>
<p>Aligning terms, we write our quadratic approximation as a weighted Gaussian problem</p>
<p><span class="math display">\[\argmin_{\beta} \ell(\beta_0) -\frac{\left(\widetilde{\bW}^{-1}(\by - g(\bX\beta_0 + \bo) - \bX\beta_0\right)^T\bW\widetilde{\bW}\bX\beta}{n} + \frac{\beta\bX^T(\bW\widetilde{\bW})\bX\beta}{2n} + \lambda P(\beta) = \argmin_{\beta} \frac{(\bz - \bX\beta)(\bW\widetilde{\bW}) (\bz - \bX\beta)}{2n} + \lambda P(\beta)\]</span></p>
<p>where <span class="math inline">\(\bz = \widetilde{\bW}^{-1}(\by - g(\bX\beta_0 + \bo)) + \bX\beta_0\)</span>. We note that this scheme is very similar to the IRLS (iteratively reweighted least squares) algorithm used to fit unpenalized GLMs, though IRLS implementations sometimes use the expected value of <span class="math inline">\(\widetilde{\bW}\)</span> at <span class="math inline">\(\beta_0\)</span> rather than the observed value, which gives them a sort of regularized quasi-Newton flavor.</p>
<p>Since this problem has been put in Gaussian form, we can re-use the coordinate descent scheme derived above, yielding our proximal Newton algorithm:</p>
<ol style="list-style-type: decimal">
<li>Initialize
<ul>
<li><span class="math inline">\(\beta = 0\)</span></li>
<li><span class="math inline">\(\eta = \bo\)</span></li>
<li><span class="math inline">\(\mu = g(\eta)\)</span></li>
<li><span class="math inline">\(\widetilde{\bw} = g'(\eta)\)</span></li>
<li><span class="math inline">\(\bz = \widetilde{\bW}^{-1}(y - g(\eta)) + \eta\)</span></li>
<li><span class="math inline">\(\br = \bz - \eta\)</span></li>
</ul>
</li>
<li>Repeat until convergence: (outer proximal Newton loop)
<ul>
<li>Precompute:
<ul>
<li><span class="math inline">\(\bu = \text{diag}(\bX^T\bW\widetilde{\bW}\bX)\)</span></li>
</ul>
</li>
<li>Repeat until convergence: (inner coordinate descent loop)
<ul>
<li>For <span class="math inline">\(i \in [p]\)</span>:
<ul>
<li>Update working residual: <span class="math inline">\(\br := \br + \bX_i\beta_i\)</span>
</li>
<li>Set <span class="math inline">\(\zeta = \br^T\bW\widetilde{\bW}\bx_i\)</span>, <span class="math inline">\(\tilde{\lambda} = n\lambda \|\beta_{g,-i}\|_1\)</span>
</li>
<li>Update: <span class="math display">\[\beta_i = \frac{1}{u_i + n\lambda} \mathcal{S}_{\tilde{\lambda}}(\zeta)\]</span>
</li>
<li>Update working residual: <span class="math inline">\(\br := \br - \bx_i^T\beta_i\)</span>
</li>
</ul>
</li>
<li>Update intercept:
<ul>
<li>Update working residual: <span class="math inline">\(\br := \br + \alpha\)</span>
</li>
<li>Update intercept: <span class="math inline">\(\alpha = \langle \br, \text{diag}(\bW\tilde{\bW})/n \rangle\)</span>
</li>
<li>Update working residual: <span class="math inline">\(\br := \br - \alpha\)</span>
</li>
</ul>
</li>
</ul>
</li>
<li>Set <span class="math inline">\(t = 1\)</span>
</li>
<li>Calculate Newton directions:
<ul>
<li><span class="math inline">\(\Delta \beta = \beta - \beta_{\text{previous}}\)</span></li>
<li><span class="math inline">\(\Delta \alpha = \alpha - \alpha_{\text{previous}}\)</span></li>
</ul>
</li>
<li>Repeat: (proximal Newton backtracking search)
<ul>
<li>Set <span class="math inline">\(\beta_{\text{new}} = \beta_{\text{previous}} + t * \Delta \beta\)</span>
</li>
<li>Set <span class="math inline">\(\alpha_{\text{new}} = \alpha_{\text{previous}} + t * \Delta \alpha\)</span>
</li>
<li>If <span class="math display">\[f(\beta_{\text{new}}, \alpha_{\text{new}}) &lt; f(\beta_{\text{previous}}, \alpha_{\text{previous}}) + 
    0.5 \left(t * \langle \nabla_{\beta = \beta_{\text{previous}}}\ell, \Delta \beta \rangle + 
              t * \langle \nabla_{\alpha = \alpha_{\text{previous}}}\ell, \Delta \alpha \rangle + 
              \lambda P(\beta_{\text{new}}) - \lambda P(\beta_{\text{previous}})\right):\]</span>
<ul>
<li><span class="math inline">\(\texttt{break}\)</span></li>
</ul>
</li>
<li>Else:
<ul>
<li>Set <span class="math inline">\(t = 0.8 * t\)</span><a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>
</li>
</ul>
</li>
</ul>
</li>
<li>Update quadratic approximation:
<ul>
<li><span class="math inline">\(\eta = \bX\beta_{\text{new}} + \alpha_{\text{new}} + \bo\)</span></li>
<li><span class="math inline">\(\mu = g(\eta)\)</span></li>
<li><span class="math inline">\(\widetilde{\bw} = g'(\eta)\)</span></li>
<li><span class="math inline">\(\bz = \widetilde{\bW}^{-1}(y - g(\eta)) + \eta\)</span></li>
<li><span class="math inline">\(\br = \bz - \eta\)</span></li>
</ul>
</li>
</ul>
</li>
</ol>
<p>Note that the gradient with respect to the intercept is simply <span class="math inline">\(\langle \bw_o, \by - g(\eta_{\text{previous}}))/n\)</span> for all of the families considered. This follows from the more general result shown above that gradients are of the form <span class="math inline">\(-\bX^T \bW (\by - g(\bX\beta + \bo + \alpha))/n\)</span> and the fact that an intercept corresponds to an “all 1” column of the design matrix.</p>
<p>If we wish to include box constraints of the form <span class="math inline">\(l_i \leq \beta_i \leq u_i\)</span>, we can simply directly impose these after each soft-thresholding step.</p>
<p>The inverse link <span class="math inline">\(g\)</span> and weighting <span class="math inline">\(g'\)</span> functions are easily calculated, by simple extensions of calculations performed above:</p>
<table class="table">
<colgroup>
<col width="15%">
<col width="39%">
<col width="45%">
</colgroup>
<thead><tr class="header">
<th align="right">Family</th>
<th align="left">Inverse Link <span class="math inline">\(g(x)\)</span>
</th>
<th align="left">Weight Function <span class="math inline">\(g'(x)\)</span>
</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">Gaussian</td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="right">Logistic</td>
<td align="left"><span class="math inline">\(\textsf{inv-logit}(x) = 1 / (1 + e^{-x})\)</span></td>
<td align="left"><span class="math inline">\((1 / (1 + e^{-x})) * (1 - 1 / (1 + e^{-x}))\)</span></td>
</tr>
<tr class="odd">
<td align="right">Poisson</td>
<td align="left"><span class="math inline">\(\text{exp}(x)\)</span></td>
<td align="left"><span class="math inline">\(\text{exp}(x)\)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="stationary-conditions" class="section level2">
<h2 class="hasAnchor">
<a href="#stationary-conditions" class="anchor"></a>Stationary Conditions</h2>
<p>In this section, we derive and re-state some of the stationarity conditions from Campbell and Allen <span class="citation">(2017)</span> with the scaling conventions used by the <code>ExclusiveLasso</code> package. Many of these are used internally to test the correctness of the package.</p>
<p>We first derive the subdifferential of the penalty: <span class="math display">\[P(\beta) = \sum_{g \in \mathcal{G}} \frac{\|\beta_g\|_1^2}{2}\]</span> We abuse notation slightly here and use <span class="math inline">\(x\)</span> and <span class="math inline">\(\{x\}\)</span> interchangeably.</p>
<p>Clearly, this separates group-wise so we first consider the single group case: <span class="math display">\[P(\beta_g) = \frac{\|\beta_g\|_1^2}{2} = \frac{1}{2} \left(\sum_{i \in g } |\beta_{i}|\right)^2\]</span></p>
<p>If <span class="math inline">\(\beta_{i}\)</span> is non-zero, then <span class="math inline">\(P\)</span> is differentiable with respect to <span class="math inline">\(\beta_i\)</span> and the subdifferential is given by the classical derivative: <span class="math display">\[\partial_{\beta_i} P(\beta_g) = \frac{1}{2} 2 *\left(|\beta_i| + \|\beta_{-i}\|_1\right) * \sign(\beta_i) = \sign(\beta_i) \|\beta_g\|_1\]</span></p>
<p>If <span class="math inline">\(\beta_{i}\)</span> is zero, the subdifferential can be evaluted similarly, using the well-known subdifferential of the absolute value operator at zero: <span class="math display">\[\partial_{\beta_i} P(\beta_g) = \frac{1}{2} * 2 * \left(|\beta_i| + \|\beta_{-i}\|_1\right) * [-1, 1] = [-\|\beta_g\|_1, \|\beta_g\|_1]\]</span> Since <span class="math inline">\(\beta_i = 0\)</span>, <span class="math inline">\(\|\beta_{-i}\|_{1} = \|\beta_g\|_1\)</span> here, which lets us simplify notation.</p>
<p>Together, this gives <span class="math display">\[\partial_{\beta_i} P(\beta) = \partial(|\beta_i|) * \|\beta_g\|_1 = \begin{cases} \sign(\beta_i) \|\beta_g\|_1 &amp; \beta_i \neq 0 \\ [-\|\beta_g\|_1, \|\beta_g\|_1] &amp;\beta_i = 0 \end{cases}\]</span></p>
<div id="proximal-operator" class="section level3">
<h3 class="hasAnchor">
<a href="#proximal-operator" class="anchor"></a>Proximal Operator</h3>
<p>From the above, we can derive KKT conditions for the proximal operator quickly: <span class="math display">\[\bz = \prox_{\lambda P(\cdot)}(\bx) = \argmin_{\bz} \frac{1}{2} \|\bx - \bz\|_2^2 + \frac{\lambda}{2}P(\bz)\]</span> giving <span class="math display">\[\bz = \prox_{\lambda P(\cdot)}(\bx) \Longleftrightarrow 0 \in (x_i - z_i) + \partial_{z_i}P(\bz)\]</span></p>
<p>If <span class="math inline">\(z_i \neq 0\)</span>, this gives the check: <span class="math display">\[0 = (x_i - z_i) + \lambda \sign(z_i)\|z_g\|_1 \implies x_i = z_i + \lambda \sign(z_i)\|z_g\|_1\]</span> for <span class="math inline">\(i \in g\)</span>. Similarly, if <span class="math inline">\(z_i = 0\)</span>, this gives the check: <span class="math display">\[0 \in (x_i - \underbrace{z_i}_{=0}) + \lambda [-\|z_g\|_1, \|z_g\|_1] \implies |x_i| \leq \lambda \|z_g\| \]</span></p>
<p>A symmetry argument makes clear that if <span class="math inline">\(\bz = \prox_{\lambda P(\cdot)}(\bx)\)</span>, then we must have <span class="math inline">\(\sign(z_i) \in \{0, \sign(x_i)\}\)</span> for all <span class="math inline">\(i\)</span>.</p>
<p>We can also derive explicit formulae for the proximal operator in a few cases:</p>
<ul>
<li>
<span class="math inline">\(\lambda = 0\)</span>: <span class="math inline">\(\prox_{0 * P(\cdot)}(\bx) = \bx\)</span>
</li>
<li>Groups of size 1: <span class="math inline">\(\prox_{\lambda P(\cdot)}(\bx) = \frac{\bx}{1+\lambda}\)</span>
</li>
</ul>
<p>Furthermore, if <span class="math inline">\(\prox_{\lambda P(\cdot)}(\bx)\)</span> has only a single non-zero element in a group, <span class="math inline">\(z_i \neq 0\)</span>, then it must satisfy <span class="math display">\[z_i = \frac{1}{1+\lambda} x_i\]</span> which generalizes the result above. This is connected to the result that that the exclusive lasso reduces to ridge regression when all groups are of size one.</p>
</div>
<div id="gaussian-exclusive-lasso" class="section level3">
<h3 class="hasAnchor">
<a href="#gaussian-exclusive-lasso" class="anchor"></a>Gaussian Exclusive Lasso</h3>
<p>For the general problem, the KKT conditions are given by: <span class="math display">\[0 \in -\bX^T(\by - \bX\beta)/n + \lambda \partial P(\beta)\]</span></p>
<p>Restricting attention to the (estimated) support <span class="math inline">\(\hat{\beta}_{\hat{S}}\)</span>, Campbell and Allen <span class="citation">(2017, Proposition 1)</span> show that <span class="math inline">\(\hat{\beta}_{\hat{S}}\)</span> has a closed-form solution, conditional on the signed support, given by <span class="math display">\[\hat{\beta}_{\hat{S}} = (\bX_{\hat{S}}^T\bX_{\hat{S}}/n + \lambda \bM_\hat{S})^{\dagger}\bX_{\hat{S}}^T\by/n\]</span> where <span class="math inline">\(\bM_{\hat{S}}\)</span> is a block-diagonal matrix<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> with blocks given by <span class="math inline">\(\text{sign}(\hat{\beta}_{\hat{S} \cap g})\text{sign}(\hat{\beta}_{\hat{S} \cap g})^T\)</span>. Note that we have an additional factor of <span class="math inline">\(1/n\)</span> due to the extra <span class="math inline">\(n^{-1}\)</span> in our problem formulation.</p>
</div>
</div>
<div id="statistical-properties" class="section level2">
<h2 class="hasAnchor">
<a href="#statistical-properties" class="anchor"></a>Statistical Properties</h2>
<div id="degrees-of-freedom" class="section level3">
<h3 class="hasAnchor">
<a href="#degrees-of-freedom" class="anchor"></a>Degrees of Freedom</h3>
<p>Under our scaling, an unbiased estimate of the degrees of freedom is given by <span class="math display">\[\hat{\text{df}} = \trace\left[\bX_{\hat{S}}\left(\bX_{\hat{S}}^T\bX_{\hat{S}} + n * \lambda \bM_{\hat{S}}\right)^{\dagger}\bX_{\hat{S}}^T\right]\]</span> where <span class="math inline">\(\bM\)</span> is as above.</p>
<p>This follows by simple substitution of <span class="math inline">\(\lambda \to \lambda * n\)</span> into Theorem 5 of Campbell and Allen <span class="citation">(2017)</span> to align the two formulations.</p>
<p>To improve numerical stability, we add a small multiple of the identity matrix to the <span class="math inline">\(\bX_{\hat{X}}^T\bX_{\hat{S}} + n * \lambda \bM_{\hat{S}}\)</span> term before taking the (pseudo)-inverse.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
<p><strong>FIXME</strong> Need to account for observation weights and intercept in DF calculation.</p>
</div>
</div>
<div id="benchmarks" class="section level2">
<h2 class="hasAnchor">
<a href="#benchmarks" class="anchor"></a>Benchmarks</h2>
<p>We implement a few benchmarks to demonstrate the speed of the coordinate-descent algorithm used in the <code>ExclusiveLasso</code> package.</p>
<div class="sourceCode" id="cb5"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="no">microbenchmark</span>)

<span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="no">glmnet</span>)
<span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="no">ncvreg</span>)
<span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="no">grpreg</span>)
<span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="no">ExclusiveLasso</span>)

<span class="no">n</span> <span class="kw">&lt;-</span> <span class="fl">200</span>
<span class="no">p</span> <span class="kw">&lt;-</span> <span class="fl">1000</span>
<span class="no">g</span> <span class="kw">&lt;-</span> <span class="fl">10</span>

<span class="no">groups</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span>(<span class="fl">1</span>:<span class="no">g</span>, <span class="kw">length.out</span><span class="kw">=</span><span class="no">p</span>)

<span class="no">Sig</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/toeplitz.html">toeplitz</a></span>(<span class="fl">1</span> + <span class="fl">0.95</span>^(<span class="fl">1</span>:<span class="no">p</span>))
<span class="no">Sig_L</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/chol.html">chol</a></span>(<span class="no">Sig</span>)

<span class="no">X</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span>(<span class="no">n</span> * <span class="no">p</span>), <span class="kw">ncol</span><span class="kw">=</span><span class="no">p</span>) <span class="kw">%*%</span> <span class="no">Sig_L</span>
<span class="no">beta</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span>(<span class="fl">0</span>, <span class="no">p</span>); <span class="no">beta</span>[<span class="fl">1</span>:<span class="no">g</span>] <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span>(<span class="no">g</span>, <span class="fl">2</span>, <span class="fl">3</span>)
<span class="no">y</span> <span class="kw">&lt;-</span> <span class="no">X</span> <span class="kw">%*%</span> <span class="no">beta</span> + <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span>(<span class="no">n</span>)

<span class="no">MB</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/microbenchmark/man/microbenchmark.html">microbenchmark</a></span>(<span class="kw">exlasso</span><span class="kw">=</span><span class="fu"><a href="../reference/exclusive_lasso.html">exclusive_lasso</a></span>(<span class="no">X</span>, <span class="no">y</span>, <span class="kw">groups</span><span class="kw">=</span><span class="no">groups</span>, <span class="kw">skip_df</span><span class="kw">=</span><span class="fl">TRUE</span>),
                     <span class="kw">glmnet</span><span class="kw">=</span><span class="fu"><a href="https://rdrr.io/pkg/glmnet/man/glmnet.html">glmnet</a></span>(<span class="no">X</span>, <span class="no">y</span>),
                     <span class="kw">mcp</span><span class="kw">=</span><span class="fu"><a href="https://rdrr.io/pkg/ncvreg/man/ncvreg.html">ncvreg</a></span>(<span class="no">X</span>, <span class="no">y</span>, <span class="kw">penalty</span><span class="kw">=</span><span class="st">"MCP"</span>),
                     <span class="kw">scad</span><span class="kw">=</span><span class="fu"><a href="https://rdrr.io/pkg/ncvreg/man/ncvreg.html">ncvreg</a></span>(<span class="no">X</span>, <span class="no">y</span>, <span class="kw">penalty</span><span class="kw">=</span><span class="st">"SCAD"</span>),
                     <span class="kw">grplasso</span><span class="kw">=</span><span class="fu"><a href="https://rdrr.io/pkg/grpreg/man/grpreg.html">grpreg</a></span>(<span class="no">X</span>, <span class="no">y</span>, <span class="kw">group</span><span class="kw">=</span><span class="no">groups</span>),
                     <span class="kw">times</span><span class="kw">=</span><span class="fl">20</span>)</pre></body></html></div>
<p>We skip the degrees of freedom calculation for the exclusive lasso for consistency with <code>ncvreg</code> and <code>grpreg</code>. (<code>glmnet</code> does return degrees of freedom, but it is essentially free for <code>l1</code> penalized models and incorrect for models with <span class="math inline">\(\ell_2\)</span> penalization.)</p>
<p>This gives the following results on a mid-range Dell XPS 2017 laptop:</p>
<table class="table">
<thead><tr class="header">
<th align="left">Method</th>
<th align="right">Median</th>
<th align="right">Lower Quartile</th>
<th align="right">Mean</th>
<th align="right">Upper Quartile</th>
<th align="right">Time (milliseconds)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left"><strong>exlasso</strong></td>
<td align="right"><strong>97.27</strong></td>
<td align="right">99.09</td>
<td align="right">95.44</td>
<td align="right">93.80</td>
<td align="right">3707.09</td>
</tr>
<tr class="even">
<td align="left"><strong>glmnet</strong></td>
<td align="right"><strong>1.00</strong></td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">38.11</td>
</tr>
<tr class="odd">
<td align="left"><strong>mcp</strong></td>
<td align="right"><strong>1.32</strong></td>
<td align="right">1.30</td>
<td align="right">1.29</td>
<td align="right">1.29</td>
<td align="right">50.34</td>
</tr>
<tr class="even">
<td align="left"><strong>scad</strong></td>
<td align="right"><strong>1.52</strong></td>
<td align="right">1.50</td>
<td align="right">1.81</td>
<td align="right">1.52</td>
<td align="right">57.77</td>
</tr>
<tr class="odd">
<td align="left"><strong>grplasso</strong></td>
<td align="right"><strong>10.64</strong></td>
<td align="right">10.80</td>
<td align="right">10.65</td>
<td align="right">10.47</td>
<td align="right">405.61</td>
</tr>
</tbody>
</table>
<p>All results are relative to <code>glmnet</code>. While the timings are not directly comparable, since <code>glmnet</code>, <code>ncvreg</code>, and <code>exclusive_lasso</code> all solve different problems, we see that the performance of all three methods is quite good due to the use of efficient coordinate descent algorithms. Not surprisingly, the exclusive lasso is slower than the methods with elementwise penalization (<code>glmnet</code>, <code>mcp</code>, and <code>scad</code>), but this is not too surprising given the significantly more complex penalty structure. In practice, the current implementation of the degrees of freedom calculation takes considerably more time than actual computation of the regularization path.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h2>
<div id="refs" class="references">
<div id="ref-Beck:2009">
<p>Beck, Amir, and Marc Teboulle. 2009. “A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems.” <em>SIAM Journal on Imaging Sciences</em> 2 (1): 183–202. <a href="https://doi.org/10.1137/080716542">https://doi.org/10.1137/080716542</a>.</p>
</div>
<div id="ref-Beck:2010">
<p>———. 2010. “Convex Optimization in Signal Processing and Communications.” In <em>Convex Optimization in Signal Processing and Communications</em>, edited by Daniel P. Palomar and Yonina C. Eldar, 42–88. Cambridge University Press.</p>
</div>
<div id="ref-Boyd:2011">
<p>Boyd, Stephen, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. 2011. “Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.” <em>Foundations and Trends in Machine Learning</em> 3 (1): 1–122. <a href="https://doi.org/10.1561/2200000016">https://doi.org/10.1561/2200000016</a>.</p>
</div>
<div id="ref-Breheny:2011">
<p>Breheny, Patrick, and Jian Huang. 2011. “Coordinate Descent Algorithms for Nonconvex Penalized Regression, with Applications to Biological Feature Selection.” <em>Annals of Applied Statistics</em> 5 (1): 232–53. <a href="https://doi.org/10.1214/10-AOAS388">https://doi.org/10.1214/10-AOAS388</a>.</p>
</div>
<div id="ref-Breheny:2015">
<p>———. 2015. “Group Descent Algorithms for Nonconvex Penalized Linear and Logistic Regression Models with Grouped Predictors.” <em>Statistics and Computing</em> 25 (2): 173–87. <a href="https://doi.org/10.1007/s11222-013-9424-2">https://doi.org/10.1007/s11222-013-9424-2</a>.</p>
</div>
<div id="ref-Byrd:2016">
<p>Byrd, Richard H., Jorge Nocedal, and Figen Oztoprak. 2016. “An Inexact Successive Quadratic Approximation Method for L1 Regularized Optimization.” <em>Mathematical Programming, Series B</em> 157: 375–96. <a href="https://doi.org/10.1007/s10107-015-0941-y">https://doi.org/10.1007/s10107-015-0941-y</a>.</p>
</div>
<div id="ref-Campbell:2017">
<p>Campbell, Frederick, and Genevera I. Allen. 2017. “Within-Group Variable Selection Through the Exclusive Lasso.” <em>Electronic Journal of Statistics</em> 11 (2): 4220–57. <a href="https://doi.org/10.1214/17-EJS1317">https://doi.org/10.1214/17-EJS1317</a>.</p>
</div>
<div id="ref-Daubechies:2004">
<p>Daubechies, Ingred, Michel Defrise, and Christine de Mol. 2004. “An Iterative Thresholding Algorithm for Linear Inverse Problems with a Sparsity Constraint.” <em>Communications on Pure and Applied Mathematics</em> 57 (11): 1413–57. <a href="https://doi.org/10.1002/cpa.20042">https://doi.org/10.1002/cpa.20042</a>.</p>
</div>
<div id="ref-Friedman:2007">
<p>Friedman, Jerome, Trevor Hastie, Holger Höfling, and Robert Tibshirani. 2007. “Pathwise Coordinate Optimization.” <em>Annals of Applied Statistics</em> 1 (2): 302–32. <a href="https://doi.org/10.1214/07-AOAS131">https://doi.org/10.1214/07-AOAS131</a>.</p>
</div>
<div id="ref-Friedman:2010">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2010. “Regularization Paths for Generalized Linear Models via Coordinate Descent.” <em>Journal of Statistical Software</em> 33 (1): 1–22. <a href="https://doi.org/10.18637/jss.v033.i01">https://doi.org/10.18637/jss.v033.i01</a>.</p>
</div>
<div id="ref-Lee:2014">
<p>Lee, Jason D., Yuekai Sun, and Michael Saunders. 2014. “Proximal Newton-Type Methods for Minimizing Composite Functions.” <em>SIAM Journal on Optimization</em> 24 (3): 1420–43. <a href="https://doi.org/10.1137/130921428">https://doi.org/10.1137/130921428</a>.</p>
</div>
<div id="ref-Obozinski:2012">
<p>Obozinski, Guillaume, and Francis Bach. 2012. “Convex Relaxation for Combinatorial Penalties.” <a href="https://arxiv.org/abs/1205.1240">https://arxiv.org/abs/1205.1240</a>.</p>
</div>
<div id="ref-Parikh:2014">
<p>Parikh, Neal, and Stephen Boyd. 2014. “Proximal Algorithms.” <em>Foundations and Trends in Optimization</em> 1 (3): 127–239. <a href="https://doi.org/10.1561/2400000003">https://doi.org/10.1561/2400000003</a>.</p>
</div>
<div id="ref-Schmidt:2011">
<p>Schmidt, Mark, Nicolas L. Roux, and Francis R. Bach. 2011. “Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization.” In <em>Advances in Neural Information Processing Systems 24 (NIPS 2011)</em>, edited by John Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett, Fernando Pereira, and Killian Q. Weinberger, 1458–66. <a href="https://papers.nips.cc/paper/4452-convergence-rates-of-inexact-proximal-gradient-methods-for-convex-optimization">https://papers.nips.cc/paper/4452-convergence-rates-of-inexact-proximal-gradient-methods-for-convex-optimization</a>.</p>
</div>
<div id="ref-Shi:2016">
<p>Shi, Hao-Jun Michael, Shenyinying Tu, Yangyang Xu, and Wotao Yin. 2016. “A Primer on Coordinate Descent Algorithms.” <a href="https://arxiv.org/abs/1610.00040">https://arxiv.org/abs/1610.00040</a>.</p>
</div>
<div id="ref-Tseng:2001">
<p>Tseng, Paul. 2001. “Convergence of a Block Coordinate Descent Method for Nondifferentiable Minimization.” <em>Journal of Optimization Theory and Applications</em> 109 (3): 475–94. <a href="https://doi.org/10.1023/A:1017501703105">https://doi.org/10.1023/A:1017501703105</a>.</p>
</div>
<div id="ref-Wu:2008">
<p>Wu, Tong Tong, and Kenneth Lange. 2008. “Coordinate Descent Algorithms for Lasso Penalized Regression.” <em>Annals of Applied Statistics</em> 2 (1): 224–44. <a href="https://doi.org/10.1214/07-AOAS147">https://doi.org/10.1214/07-AOAS147</a>.</p>
</div>
<div id="ref-Zhou:2010">
<p>Zhou, Yang, Rong Jin, and Steven C. H. Hoi. 2010. “Exclusive Lasso for Multi-Task Feature Selection.” In <em>AISTATS 2010: Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</em>, edited by Yee Whye Teh and Mike Titterington. JMLR. <a href="http://proceedings.mlr.press/v9/zhou10a.html">http://proceedings.mlr.press/v9/zhou10a.html</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>Below, when observation weights are included, they will be included in the definition of <span class="math inline">\(\ell\)</span>, but not of the <span class="math inline">\(l_i\)</span> terms.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>As will be shown below, the proximal operator of the Exclusive Lasso problem is non-trivial and somewhat expensive to evaluate to high precision. For this reason, an accelerated proximal gradient (“FISTA”) algorithm may be implemented in a future version of the <code>ExclusiveLasso</code> package.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>We can also take advantage of the group-structure to evaluate the proximal operator in parallel, though this is currently not implemented in the <code>ExclusiveLasso</code> package.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>In particular, we restrict our attention to canonical link functions, natural parameterizations, and non-overdispersed sampling distributions.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>The value of <span class="math inline">\(\alpha\)</span> here defaults to <span class="math inline">\(0.8\)</span> and is controlled by the pre-processor macro <code>EXLASSO_BACKTRACK_BETA</code>.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>The step-size decrease rate can be controlled by the pre-processor macro <code>EXLASSO_BACKTRACK_BETA</code>.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p><span class="math inline">\(\bM_{\hat{S}}\)</span> may be a <em>permuted</em> block-diagonal matrix if the group structure does not correspond to adjacent columns of <span class="math inline">\(\bX\)</span>.<a href="#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p> After adding this term, this matrix is always strictly positive definite so the pseudo-inverse becomes the standard inverse.<a href="#fnref8" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Michael Weylandt, Frederick Campbell, Genevera Allen.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.5.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
