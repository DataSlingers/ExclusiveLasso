% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ExclusiveLasso.R
\name{exclusive_lasso}
\alias{exclusive_lasso}
\title{Fit a GLM with Exclusive Lasso Regularization}
\usage{
exclusive_lasso(
  X,
  y,
  groups,
  family = c("gaussian", "binomial", "poisson"),
  weights,
  offset,
  nlambda = 100,
  lambda.min.ratio = ifelse(nobs < nvars, 0.01, 1e-04),
  lambda,
  standardize = TRUE,
  intercept = TRUE,
  lower.limits = rep(-Inf, nvars),
  upper.limits = rep(Inf, nvars),
  thresh = 1e-07,
  thresh_prox = thresh,
  skip_df = FALSE,
  algorithm = c("cd", "pg")
)
}
\arguments{
\item{X}{The matrix of predictors (\eqn{X \in \R^{n \times p}}{X})}

\item{y}{The response vector (\eqn{y})}

\item{groups}{An integer vector of length \eqn{p} indicating group membership.
(Cf. the \code{group} argument of \code{\link[grpreg]{grpreg}})}

\item{family}{The GLM response type. (Cf. the \code{family} argument of
\code{\link[stats]{glm}})}

\item{weights}{Weights applied to individual
observations. If not supplied, all observations will be equally
weighted. Will be re-scaled to sum to \eqn{n} if
necessary. (Cf. the \code{weight} argument of
\code{\link[stats]{lm}})}

\item{offset}{A vector of length \eqn{n} included in the linear
predictor.}

\item{nlambda}{The number of lambda values to use in computing the
regularization path. Note that the time to run is typically sublinear
in the grid size due to the use of warm starts.}

\item{lambda.min.ratio}{The smallest value of lambda to be used, as a fraction
of the largest value of lambda used. Unlike the lasso, there is no
value of lambda such that the solution is wholly sparse, but we still
use lambda_max from the lasso.}

\item{lambda}{A user-specified sequence of lambdas to use.}

\item{standardize}{Should \code{X} be centered and scaled before fitting?}

\item{intercept}{Should the fitted model have an (unpenalized) intercept term?}

\item{lower.limits}{A vector of lower bounds for each coefficient (default \code{-Inf}).
Can either be a scalar (applied to each coefficient) or a vector
of length \code{p} (number of coefficients).}

\item{upper.limits}{A vector of lower bounds for each coefficient (default \code{Inf}).
Can either be a scalar (applied to each coefficient) or a vector
of length \code{p} (number of coefficients).}

\item{thresh}{The convergence threshold used for the proximal
gradient or coordinate-descent algorithm used to solve the
penalized regression problem.}

\item{thresh_prox}{The convergence threshold used for the
coordinate-descent algorithm used to evaluate the proximal operator.}

\item{skip_df}{Should the DF calculations be skipped? They are often slower
than the actual model fitting; if calling \code{exclusive_lasso} repeatedly
it may be useful to skip these calculations.}

\item{algorithm}{Which algorithm to use, proximal gradient (\code{"pg"}) or
coordinate descent (\code{"cd"})? Empirically, coordinate descent appears
to be faster for most problems (consistent with Campbell and Allen), but
proximal gradient may be faster for certain problems with many small groups
where the proximal operator may be evaluated quickly and to high precision.}
}
\value{
An object of class \code{ExclusiveLassoFit} containing \itemize{
\item \code{coef} - A matrix of estimated coefficients
\item \code{intercept} - A vector of estimated intercepts if \code{intercept=TRUE}
\item \code{X, y, groups, weights, offset} - The data used to fit the model
\item \code{lambda} - The vector of \eqn{\lambda}{lambda} used
\item \code{df} - An unbiased estimate of the degrees of freedom (see Theorem
      5 in [1])
\item \code{nnz} - The number of non-zero coefficients at each value of
      \eqn{\lambda}{lambda}
}
}
\description{
Fit a generalized linear model via maximum penalized likelihood
using the exclusive lasso penalty. The regularization path is computed
along a grid of values for the regularization parameter (lambda).
The interface is intentionally similar to that of \code{\link[glmnet]{glmnet}} in
the package of the same name.
}
\details{
Note that unlike Campbell and Allen (2017), we use the "1/n"-scaling of the
loss function.

For the Gaussian case:
\deqn{\frac{1}{2n}|y - X\beta|_2^2 + \lambda P(\beta, G)}

For other GLMs:
\deqn{-\frac{1}{n}\ell(y, X\beta)+ \lambda P(\beta, G)}

By default, an optimized implementation is used for \code{family="gaussian"}
         which is approximately 2x faster for most problems. If you wish
         to disable this code path and use the standard GLM implementation
         with Gaussian response, set \code{options(ExclusiveLasso.gaussian_fast_path=FALSE).}
}
\examples{
n <- 200
p <- 500
groups <- rep(1:10, times=50)
beta <- numeric(p);
beta[1:10] <- 3

X <- matrix(rnorm(n * p), ncol=p)
y <- X \%*\% beta + rnorm(n)

exfit <- exclusive_lasso(X, y, groups)
}
\references{
Campbell, Frederick and Genevera I. Allen. "Within Group Variable Selection
    with the Exclusive Lasso". Electronic Journal of Statistics 11(2),
    pp.4220-4257. 2017. \doi{10.1214/17-EJS1317}
}
